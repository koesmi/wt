\documentclass{article}
\usepackage[a4paper,margin=1.875in,top=1.875in,bottom=1.875in]{geometry}

\usepackage{amsmath,mathtools,bbm,amssymb}
\usepackage[german]{babel}

\usepackage{setspace}
\doublespacing

\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{0pt} 
\pagestyle{fancy}
\lhead{Blatt 6 Nicolas und Evgenij}\rhead{Seite \thepage}
\fancyfoot{}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,arrows.meta}

\usepackage[numbers]{natbib}
\bibliographystyle{alphadin}
\usepackage{url}
\usepackage{hyperref}

\begin{document}

\paragraph{B6A1}
Seien $X_1,X_2,\dots$ \emph{iid} uniform auf $[0,1]$ verteilt.
Weiter sei $f\in L^1([0,1])$.
Zeigen Sie, dass die Monte-Carlo Simulation $\hat{I}_n:=\frac{1}{n}\sum_{i=1}^nf(X_i)$ fast sicher gegen das Integral $\int_0^1f(x)\mathrm{d}x$ konvergiert.

Beispiel 5.21 in \cite{klenke}, funktioniert mit starkem Gesetz der großen Zahlen.
Betrachte hierzu die Zufallsvariablen $f(X_i)$.
Das Starke Gesetz der großen Zahlen gemäß Theorem 51 lautet hierfür, dass für reellwertige unabhängige und identisch verteilte Zufallsvariablen mit $E[|f(X_1)|]<\infty$ gilt, dass
\[
  \frac{1}{n}\sum_{i=1}^nf(X_i)\to E[f(X_1)]=\int_0^1f(x)\mathrm{d}P
  =\int_0^1f(x)\mathrm{d}x
\]
Nach dem Blockungslemma 30 sind diese ebenfalls unabhängig.
Die $f(X_i)$ sind ebenfalls identisch verteilt, denn für alle $i\in\mathbb{N}$ gilt $(f\circ X_i)_\# P=P\circ X_i^{-1}\circ f^{-1}=P\circ X_1^{-1}\circ f^{-1}$, weil die $X_i$ identisch verteilt sind.
Somit können wir das starke Gesetz der großen Zahlen mit $L^1$-Voraussetzung, quasi Theorem 51, benutzen.
Nach diesem gilt
\begin{align*}
  \lim_{n\to\infty}\hat{I}_N
  &=\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^nf(X_i)
    E[f(X_1)]\,.
    \intertext{Mit der Definition des Erwartungswertes folgt}
  &=\int f\circ X_1\mathrm{d}P\,.
    \intertext{weil $X_1$ uniform auf $[0,1]$ verteilt ist, kriegen wir}
  &=\int_{0}^1f\circ\mathrm{id}(x)\lambda(\mathrm{d}x)=\int_{0}^1f(x)\mathrm{d}x\,.
\end{align*}
\newpage

\paragraph{B6A2}
Für jedes $n\in\mathbb{N}$ seien $X_1^{(n)},\dots,X_n^{(n)}$ paarweise unkorrelierte Zufallsvariablen mit endlicher Varianz (also nicht notwendig identisch verteilt) und
\[
\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n\operatorname{Var}\bigl[X_i^{(n)}\bigr]=0\,.
\]
Zeigen Sie, dass die $X_i^{(n)}$ dem schwachen Gesetz der großen Zahlen genügen, d.h. beweisen Sie
\[
\frac{1}{n}\sum_{i=1}^n\bigl(X_i^{(n)}-E\bigl[X_i^{(n)}\bigr]\bigr)\xrightarrow{P}0,\quad n\to\infty\,.
\]
Seien $\varepsilon,\delta>0$ gegeben und betrachte für jedes $n\in\mathbb{N}$ die Folge $(Y_i)_{i\in\mathbb{N}}$ gegeben durch
\[
  Y_i=
  \begin{cases}
    \frac{X_i^{(n)}}{n}&\text{ falls $i\leq n$ und}\\
    0&\text{ sonst}\,.
  \end{cases}
\]
Da $X_i^{(n)}\in L^2$ gilt auch $Y_i\in L^2$.
Da nach Aufgabenstellung gilt, dass $\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n\operatorname{Var}\bigl[X_i^{(n)}\bigr]=\lim_{n\to\infty}\sum_{i=1}^n\operatorname{Var}\bigl[Y_i\bigr]=0$, kann ich $n_0$ so wählen, dass für alle $n\geq n_0$ gilt $\frac{1}{\varepsilon^2}\sum_{i=1}^n\operatorname{Var}(Y_i)<\delta$.
Wir haben dann mit der Definition von $(Y_i)_i$
\begin{align*}
  P\Bigl(\frac{1}{n}\sum\nolimits_{i=1}^n\bigl(X_i^{(n)}-E[X_i^{(n)}]\bigr)>\varepsilon\Bigr)
  &=P\Bigl(\sup_{n\in\mathbb{N}}\Bigl|\sum\nolimits_{i=1}^nY_i-E[Y_i]\Bigr|>\varepsilon\Bigr)\,,
    \intertext{sowie nach der Maximalungleichung aus Satz 42}
  &\leq \frac{\sum_{i=1}^\infty\operatorname{Var}(Y_i)}{\varepsilon^2}\,.
    \intertext{Nach der Wahl von $n_0$ gilt für alle $n\geq n_0$, dass $\frac{1}{\varepsilon^2}\sum_{i=1}^n\operatorname{Var}(Y_i)<\delta$.
    Somit gilt insgesamt schließlich}
    <\delta\,,
\end{align*}
sodass $\frac{1}{n}\sum_{i=1}^n(X_i^{(n)}-E[X_i^{(n)}])\xrightarrow{P}0$.

Es sei $(X_n)_{n\geq2}$ eine Folge unabhängiger Zufallsvariablen mit
\[
P(X_n=n)=\frac{1}{n\log n}\quad\text{und}\quad P(X_n=0)=1-\frac{1}{n\log n}\,.
\]
Zeigen Sie, dass die Folge dem schwachen Gesetz der großen Zahlen genügt, in dem Sinne, dass
\[
\frac{1}{n}\sum_{i=2}^n(X_i-E[X_i])\xrightarrow{P}0\,.
\]

%https://math.stackexchange.com/questions/1288502/sequence-satisfies-weak-law-of-large-numbers-but-doesnt-satisfy-strong-law-of-l
%https://dspace.mit.edu/bitstream/handle/1721.1/70477/6-042j-fall-2002/contents/lecture-notes/cp15Msol.pdf
Wir wollen für ein festes $n\in\mathbb{N}$ die Tschebyscheff-Ungleichung für die Zufallsvariable $\frac{1}{n}\sum_{i=2}^nX_i$ verwenden.
Hierfür brauchen wir die Varianz von $\frac{1}{n}\sum_{i=2}^nX_i$.
Da die $X_i$ unabhängig sind, gilt für diese
\begin{align*}
  \operatorname{Var}\left(\frac{1}{n}\sum\nolimits_{i=2}^nX_i\right)
  &=\frac{1}{n^2}\sum\nolimits_{i=2}^n\operatorname{Var}(X_i)\,.
    \intertext{Nach den Rechenregeln für die Varianz können wir schreiben}
  &=\frac{1}{n^2}\sum\nolimits_{i=2}^n\Bigl(E[X_i^2]-X[X_i]^2\Bigr)\,.
    \intertext{Für den Erwartungswert sind nur die Wahrscheinlichkeiten mit $X_i=i$ zu betrachten, sodass sich ergibt}
  &=\frac{1}{n^2}\sum\nolimits_{i=2}^n\left(\frac{i}{\log i}-\frac{1}{\log i}\right)=\frac{1}{n^2}\sum\nolimits_{i=1}^n\frac{i}{\log(i+1)}\,.
    \intertext{Nun gilt für $i\geq 1$, dass $i>\log(i+1)$.
    Deshalb gilt auch $\frac{n}{\log(n)}\geq\frac{i}{\log(i+1)}$ und ich kann abschätzen}
    &\leq \frac{1}{n^2}\sum\nolimits_{i=1}^n\frac{n}{\log n}=\frac{1}{n^2}\cdot n\cdot\frac{n}{\log n}=\frac{1}{\log n}\,.
\end{align*}
Nach der Tschebyscheff-Ungleichung gilt damit für jedes $\varepsilon>0$ und jedes $n\in\mathbb{N}$, dass $P\bigl(\frac{1}{n}\sum_{i=2}^n(X_i-E[X_i])>\varepsilon\bigr)\leq\frac{1}{n^2}\frac{1}{\varepsilon^2}\operatorname{Var}(X_i)\xrightarrow{n\to\infty}0$.

Zeigen Sie weiter, dass die obige Folge nicht fast sicher konvergiert und sie somit nicht dem Gesetz der großen Zahlen genügt.
Verwenden Sie dazu das Lemma von Borel--Cantelli.

Wir nutzen den Tipp und betrachten $\sum_{n=2}^\infty P(X_n=n)=\sum_{n=2}^\infty\frac{1}{n\log n}$.
Nach dem Integraltest divergiert die Reihe, wenn das dazugehörige Integral $\int_2^\infty\frac{1}{x\log x}\mathrm{d}x$ konvergiert.
Durch Substituieren mit $z=\log(x)$, sodass $\frac{\mathrm{d}z}{\mathrm{d}x}=\frac{1}{x}$ und somit $\mathrm{d}z=\frac{\mathrm{d}x}{x}$ gilt, dass $\int_{2}^\infty\frac{1}{x\log x}\mathrm{d}x=\int_{\log 2}^\infty\frac{\mathrm{d}z}{z}=\Bigl.\log\log x\Bigr|_2^\infty$, sodass insgesamt $\sum_{n=2}^\infty P(X_n=n)=\infty$.
Mit dem Lemma von Borel--Cantelli gilt dann $P(\limsup\{X_n=n\})=P\bigl(\bigr)=1$.
\emph{Hier fehlt noch irgendeine Überlegung, vermutlich unter Verwendung von einer Abschätzung für die Summe.
Was gilt für die Summe, wenn $X_n=n$?}
Insgesamt erhalten wir, dass $P\bigl(\lim_{n\to\infty}\frac{1}{n}\sum_{i=2}^n(X_i-E[X_i])=0\bigr)<1$.
\newpage

\paragraph{B6A3}
Seien $X_1,X_2,\dots$ unabhängige Zufallsvariablen mit $E[X_n]=0$ für jedes $n\in\mathbb{N}$ und $V:=\sup\{\operatorname{Var}[X_n]:n\in\mathbb{N}\}<\infty$.
Definiere $S_n=X_1+\cdots+X_n$.
Dann gilt für jedes $\varepsilon>0$
\[
  \limsup_{n\to\infty}\frac{|S_n|}{n^{1/2}(\log(n))^{1/2+\varepsilon}}=0\quad\text{fast sicher.}
\]
\emph{Hinweis: Definieren Sie $k_n=2^n$ und $l(n)=n^{1/2}(\log(n))^{1/2+\varepsilon}$ für $n\in\mathbb{N}$ und betrachten Sie $l(k_{n+1})/l(k_n)$.
  Zeigen Sie, dass für hinreichend großes $n$ und für $k\in\mathbb{N}$ mit $k_{n-1}\leq k\leq k_n$ gilt $\frac{|S_k|}{l(k)}\leq\frac{2|S_k|}{l({k_n})}$.
Verwenden Sie nun die Kolmogorov'sche Ungleichung und Borel--Cantelli, um zu zeigen, dass für beliebiges $\delta>0$ gilt, dass $\limsup_{n\to\infty}l(k_n)^{-1}\max\{|S_k|: k\leq k_n\}\leq\delta$ fast sicher.}
\newpage

\paragraph{B6A4}
Beweisen Sie folgende Aussagen
\begin{enumerate}
\item[1.] Sei $(a_n)_{n\in\mathbb{N}}$ eine Folge mit $a_n\geq0$ und $\sum_{n=1}^\infty a_n<\infty$, dann folgt $\lim_{k\to\infty}\sum_{n=k}^\infty a_n=0$.
\item[2.] Sei $(a_n)_{n\in\mathbb{N}}$ eine monotone Folge und es gebe eine Teilfolge $(a_{n_k})_{k\in\mathbb{N}}$, sodass $a_{n_k}\to a$, dann folgt $a_n\to a$.
\end{enumerate}
\emph{Hinweis: Diese zwei Aussagen wurden im Beweis von Lemma 43 verwendet.
Es ist sinnvoll, diesen nach dem Bearbeiten der Übungsaufgabe zu wiederholen.}
\newpage


\bibliography{../../../books/wt}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% ispell-local-dictionary: "german"
%%% TeX-master: t
%%% End:
