\documentclass{article}
\usepackage[a4paper,margin=1.875in,top=1.875in,bottom=1.875in]{geometry}

\usepackage{amsmath,mathtools,bbm,amssymb}
\usepackage[german]{babel}

\usepackage{setspace}
\doublespacing

\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{0pt} 
\pagestyle{fancy}
\lhead{Blatt 6 Nicolas und Evgenij}\rhead{Seite \thepage}
\fancyfoot{}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,arrows.meta}

\usepackage[numbers]{natbib}
\bibliographystyle{alphadin}
\usepackage{url}
\usepackage{hyperref}

\begin{document}

Lukas, wenn irgendwas so gut ist, dass wir es vorrechnen dürfen, wäre es richtig super, wenn du das schon vor dem Tutorat kurz per Mail an koesmi@gmail.com schreibst -- dass wir bei der Aufgabe nochmal vorher drauf schauen.
Oder du sagst es im Tutorat, das geht auch :).
\paragraph{B6A1}
Seien $X_1,X_2,\dots$ \emph{iid} uniform auf $[0,1]$ verteilt.
Weiter sei $f\in L^1([0,1])$.
Zeigen Sie, dass die Monte-Carlo Simulation $\hat{I}_n:=\frac{1}{n}\sum_{i=1}^nf(X_i)$ fast sicher gegen das Integral $\int_0^1f(x)\mathrm{d}x$ konvergiert.

Beispiel 5.21 in \cite{klenke}, funktioniert mit starkem Gesetz der großen Zahlen.
Betrachte hierzu die Zufallsvariablen $f(X_i)$.
Das Starke Gesetz der großen Zahlen gemäß Theorem 51 lautet hierfür, dass für reellwertige unabhängige und identisch verteilte Zufallsvariablen mit $E[|f(X_1)|]<\infty$ gilt, dass
\[
  \frac{1}{n}\sum_{i=1}^nf(X_i)\to E[f(X_1)]=\int_0^1f(x)\mathrm{d}P
  =\int_0^1f(x)\mathrm{d}x
\]
Nach dem Blockungslemma 30 sind diese ebenfalls unabhängig.
Die $f(X_i)$ sind ebenfalls identisch verteilt, denn für alle $i\in\mathbb{N}$ gilt $(f\circ X_i)_\# P=P\circ X_i^{-1}\circ f^{-1}=P\circ X_1^{-1}\circ f^{-1}$, weil die $X_i$ identisch verteilt sind.
Somit können wir das starke Gesetz der großen Zahlen mit $L^1$-Voraussetzung, quasi Theorem 51, benutzen.
Nach diesem gilt
\begin{align*}
  \lim_{n\to\infty}\hat{I}_N
  &=\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^nf(X_i)
    E[f(X_1)]\,.
    \intertext{Mit der Definition des Erwartungswertes folgt}
  &=\int f\circ X_1\mathrm{d}P\,.
    \intertext{weil $X_1$ uniform auf $[0,1]$ verteilt ist, kriegen wir}
  &=\int_{0}^1f\circ\mathrm{id}(x)\lambda(\mathrm{d}x)=\int_{0}^1f(x)\mathrm{d}x\,.
\end{align*}
\newpage

\paragraph{B6A2}
Für jedes $n\in\mathbb{N}$ seien $X_1^{(n)},\dots,X_n^{(n)}$ paarweise unkorrelierte Zufallsvariablen mit endlicher Varianz (also nicht notwendig identisch verteilt) und
\[
\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n\operatorname{Var}\bigl[X_i^{(n)}\bigr]=0\,.
\]
Zeigen Sie, dass die $X_i^{(n)}$ dem schwachen Gesetz der großen Zahlen genügen, d.h. beweisen Sie
\[
\frac{1}{n}\sum_{i=1}^n\bigl(X_i^{(n)}-E\bigl[X_i^{(n)}\bigr]\bigr)\xrightarrow{P}0,\quad n\to\infty\,.
\]
Seien $\varepsilon,\delta>0$ gegeben und betrachte für jedes $n\in\mathbb{N}$ die Folge $(Y_i)_{i\in\mathbb{N}}$ gegeben durch
\[
  Y_i=
  \begin{cases}
    \frac{X_i^{(n)}}{n}&\text{ falls $i\leq n$ und}\\
    0&\text{ sonst}\,.
  \end{cases}
\]
Da $X_i^{(n)}\in L^2$ gilt auch $Y_i\in L^2$.
Da nach Aufgabenstellung gilt, dass $\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n\operatorname{Var}\bigl[X_i^{(n)}\bigr]=\lim_{n\to\infty}\sum_{i=1}^n\operatorname{Var}\bigl[Y_i\bigr]=0$, kann ich $n_0$ so wählen, dass für alle $n\geq n_0$ gilt $\frac{1}{\varepsilon^2}\sum_{i=1}^n\operatorname{Var}(Y_i)<\delta$.
Wir haben dann mit der Definition von $(Y_i)_i$
\begin{align*}
  P\Bigl(\frac{1}{n}\sum\nolimits_{i=1}^n\bigl(X_i^{(n)}-E[X_i^{(n)}]\bigr)>\varepsilon\Bigr)
  &=P\Bigl(\sup_{n\in\mathbb{N}}\Bigl|\sum\nolimits_{i=1}^nY_i-E[Y_i]\Bigr|>\varepsilon\Bigr)\,,
    \intertext{sowie nach der Maximalungleichung aus Satz 42}
  &\leq \frac{\sum_{i=1}^\infty\operatorname{Var}(Y_i)}{\varepsilon^2}\,.
    \intertext{Nach der Wahl von $n_0$ gilt für alle $n\geq n_0$, dass $\frac{1}{\varepsilon^2}\sum_{i=1}^n\operatorname{Var}(Y_i)<\delta$.
    Somit gilt insgesamt schließlich}
    <\delta\,,
\end{align*}
sodass $\frac{1}{n}\sum_{i=1}^n(X_i^{(n)}-E[X_i^{(n)}])\xrightarrow{P}0$.

Es sei $(X_n)_{n\geq2}$ eine Folge unabhängiger Zufallsvariablen mit
\[
P(X_n=n)=\frac{1}{n\log n}\quad\text{und}\quad P(X_n=0)=1-\frac{1}{n\log n}\,.
\]
Zeigen Sie, dass die Folge dem schwachen Gesetz der großen Zahlen genügt, in dem Sinne, dass
\[
\frac{1}{n}\sum_{i=2}^n(X_i-E[X_i])\xrightarrow{P}0\,.
\]

%https://math.stackexchange.com/questions/1288502/sequence-satisfies-weak-law-of-large-numbers-but-doesnt-satisfy-strong-law-of-l
%https://dspace.mit.edu/bitstream/handle/1721.1/70477/6-042j-fall-2002/contents/lecture-notes/cp15Msol.pdf
Wir wollen für ein festes $n\in\mathbb{N}$ die Tschebyscheff-Ungleichung für die Zufallsvariable $\frac{1}{n}\sum_{i=2}^nX_i$ verwenden.
Hierfür brauchen wir die Varianz von $\frac{1}{n}\sum_{i=2}^nX_i$.
Da die $X_i$ unabhängig sind, gilt für diese
\begin{align*}
  \operatorname{Var}\left(\frac{1}{n}\sum\nolimits_{i=2}^nX_i\right)
  &=\frac{1}{n^2}\sum\nolimits_{i=2}^n\operatorname{Var}(X_i)\,.
    \intertext{Nach den Rechenregeln für die Varianz können wir schreiben}
  &=\frac{1}{n^2}\sum\nolimits_{i=2}^n\Bigl(E[X_i^2]-X[X_i]^2\Bigr)\,.
    \intertext{Für den Erwartungswert sind nur die Wahrscheinlichkeiten mit $X_i=i$ zu betrachten, sodass sich ergibt}
  &=\frac{1}{n^2}\sum\nolimits_{i=2}^n\left(\frac{i}{\log i}-\frac{1}{\log i}\right)=\frac{1}{n^2}\sum\nolimits_{i=1}^n\frac{i}{\log(i+1)}\,.
    \intertext{Nun gilt für $i\geq 1$, dass $i>\log(i+1)$.
    Deshalb gilt auch $\frac{n}{\log(n)}\geq\frac{i}{\log(i+1)}$ und ich kann abschätzen}
    &\leq \frac{1}{n^2}\sum\nolimits_{i=1}^n\frac{n}{\log n}=\frac{1}{n^2}\cdot n\cdot\frac{n}{\log n}=\frac{1}{\log n}\,.
\end{align*}
Nach der Tschebyscheff-Ungleichung gilt damit für jedes $\varepsilon>0$ und jedes $n\in\mathbb{N}$, dass $P\bigl(\frac{1}{n}\sum_{i=2}^n(X_i-E[X_i])>\varepsilon\bigr)\leq\frac{1}{n^2}\frac{1}{\varepsilon^2}\operatorname{Var}(X_i)\xrightarrow{n\to\infty}0$.

Zeigen Sie weiter, dass die obige Folge nicht fast sicher konvergiert und sie somit nicht dem Gesetz der großen Zahlen genügt.
Verwenden Sie dazu das Lemma von Borel--Cantelli.

Wir nutzen den Tipp und betrachten $\sum_{n=2}^\infty P(X_n=n)=\sum_{n=2}^\infty\frac{1}{n\log n}$.
Nach dem Integraltest divergiert die Reihe, wenn das dazugehörige Integral $\int_2^\infty\frac{1}{x\log x}\mathrm{d}x$ konvergiert.
Durch Substituieren mit $z=\log(x)$, sodass $\frac{\mathrm{d}z}{\mathrm{d}x}=\frac{1}{x}$ und somit $\mathrm{d}z=\frac{\mathrm{d}x}{x}$ gilt, dass $\int_{2}^\infty\frac{1}{x\log x}\mathrm{d}x=\int_{\log 2}^\infty\frac{\mathrm{d}z}{z}=\Bigl.\log\log x\Bigr|_2^\infty$, sodass insgesamt $\sum_{n=2}^\infty P(X_n=n)=\infty$.
Mit dem Lemma von Borel--Cantelli gilt dann $P(\limsup\{X_n=n\})=1$.
\emph{Hier fehlt noch irgendeine Überlegung, vermutlich unter Verwendung von einer Abschätzung für die Summe.
  Was gilt für die Summe, wenn $X_n=n$?
  Siehe am Besten die Abgabe auf dem handschriftlichen Blatt.}
Insgesamt erhalten wir, dass $P\bigl(\lim_{n\to\infty}\frac{1}{n}\sum_{i=2}^n(X_i-E[X_i])=0\bigr)<1$.
\newpage

\paragraph{B6A3}
Seien $X_1,X_2,\dots$ unabhängige Zufallsvariablen mit $E[X_n]=0$ für jedes $n\in\mathbb{N}$ und $V:=\sup\{\operatorname{Var}[X_n]:n\in\mathbb{N}\}<\infty$.
Definiere $S_n=X_1+\cdots+X_n$.
Dann gilt für jedes $\varepsilon>0$
\[
  \limsup_{n\to\infty}\frac{|S_n|}{n^{1/2}(\log(n))^{1/2+\varepsilon}}=0\quad\text{fast sicher.}
\]
\emph{Hinweis: Definieren Sie $k_n=2^n$ und $l(n)=n^{1/2}(\log(n))^{1/2+\varepsilon}$ für $n\in\mathbb{N}$ und betrachten Sie $l(k_{n+1})/l(k_n)$.
  Zeigen Sie, dass für hinreichend großes $n$ und für $k\in\mathbb{N}$ mit $k_{n-1}\leq k\leq k_n$ gilt $\frac{|S_k|}{l(k)}\leq\frac{2|S_k|}{l({k_n})}$.
  Verwenden Sie nun die Kolmogorov'sche Ungleichung und Borel--Cantelli, um zu zeigen, dass für beliebiges $\delta>0$ gilt, dass $\limsup_{n\to\infty}l(k_n)^{-1}\max\{|S_k|: k\leq k_n\}\leq\delta$ fast sicher.}

Das ist Satz 5.29 in \cite{klenke}.
Zu $l(k_{n+1})/l(k_n)$ ist zu sagen, dass $l(k_{n+1})/l(k_n)>1$, denn $\log$ und $n\mapsto 2^n$ sind monoton steigend.
Genauer gilt,
\begin{align*}
  \lim_{n\to\infty}l(k_{n+1})/l(k_n)
  &=\lim_{n\to\infty}\frac{(2^{n+1})^{1/2}}{(2^n)^{1/2}}\frac{\bigl(\log(2^{n+1})\bigr)^{1/2+\varepsilon}}{\bigl(\log(2^{n})\bigr)^{1/2+\varepsilon}}\,.
    \intertext{Durch Kürzen, sowie mit den Rechenregeln für den Logarithmus ergibt sich}
  &=\lim_{n\to\infty}2^{1/2}\left(\frac{\log(2^n)+\log(2)}{\log(2^n)}\right)^{1/2+\varepsilon}\,.
    \intertext{Da $\lim_n\log(2)/\log(2^n)=0$ erhalten wir}
  &=\sqrt{2}\,,
\end{align*}
wobei die Folge von oben gegen den Limes strebt.
Damit gilt für ein hinreichend großes $n\in\mathbb{N}$ sicher, dass $l(k_{n})/l(k_{n-1})\leq2$ und entsprechend auch für alle $k\in\mathbb{N}$ so, dass $k_{n-1}\leq k\leq k_n$, dass $l({k_n})/l(k)\leq2$.
Umgestellt und mit $|S_k|$ multipliziert können wir schreiben $|S_k|/l(k)\leq2|S_k|/l(k_n)$.
Die linke Seite der Ungleichung ist das, was in der Aufgabenstellung steht.
Wenn wir nun also die Gleichung aus der Aufgabe für $k_n$ statt $n$ im Nenner zeigen, dann folgt aus der Ungleichung die Aussage auch für $n$ im Nenner, so, wie in der Aufgabe.
Außerdem, wenn der $\limsup$ gleich 0 sein soll, können wir äquivalent zeigen, dass er für ein beliebiges $\delta>0$ kleiner ist als das gewählte $\delta$ sein soll.
Sei also ein $\delta>0$ gegeben.
Statt der Aufgabenstellung reicht es, wie oben erklärt, zu zeigen, das
\[\limsup_{n\to\infty}\frac{\max_{k\leq 2^n}|S_k|}{(2^n)^{1/2}(\log(2^n))^{1/2+\varepsilon}}=0\quad\text{fast sicher.}\]
Das zeigen wir indem wir Borell--Cantelli auf $\{\max_{k\leq k_n}|S_k|>\delta l(k_n)\}$ anwenden.
Hierfür wiederum wollen wir die Kolmogorov'sche Ungleichung anwenden, so, wie sie als Satz 5.28 in \cite{klenke} steht.
Hiernach gilt
\[
P\Bigl(\max_{k\leq m}\{|S_k|\}\geq\varepsilon\Bigr)\leq\frac{\operatorname{Var}(S_m)}{\varepsilon^2}\,.
\]
Wählen wir $m=2^n$ und $\varepsilon=\delta l(2^n)$ und nutzen die $\sigma$-Additivität von $P$ sowie die Linearität der Varianz unabhängiger Zufallsvariablen, sodass sich ergibt
\begin{align*}
  \sum_{n=1}^\infty P(\max_{k\leq 2^n}>\delta l(2^n))
  &\leq\sum_{n=1}^\infty\frac{\operatorname{Var}(X_n)}{\delta^22^n(\log(2^n))^{1+2\varepsilon}}\,.
    \intertext{Nun können wir jedes $\operatorname{Var}(X_n)$ mit $\sup\operatorname{Var}(X_n)<\infty$ nach Aufgabenstellung abschätzen.
    Außerdem können wir den Ausdruck vereinfachen, wenn wir mit der Summe abschätzen, wo jeder Summand mit $2^n>0$ multipliziert ist, sodass}
  &\leq\frac{1}{\delta^2}\sup\operatorname{Var}(X_n)\sum_{n=1}^\infty\frac{2^n}{2^n(\log(2^n))^{1+2\varepsilon}}\,.
    \intertext{Schließlich können wir Logarithmusrechenregeln anwenden und das $n$ aus dem Exponenten vom Argument vom Logarithmus rausziehen, sodass}
  &=\frac{\sup\operatorname{Var}(X_n)}{\delta^2(\log 2)^{1+2\varepsilon}}\sum_{n=1}^\infty\frac{1}{n^{1+2\varepsilon}}<\infty\,,
    \intertext{denn die Varianz ist kleiner als Unendlich nach Aufgabenstellung, der Nenner verschwindet nicht und die Summe ist eine geometrische Reihe mit einem Exponent, der kleiner als $-1$ ist.}
\end{align*}
Wie erwähnt, liefert das Lemma von Borel--Cantelli die Aussage.
\newpage

\paragraph{B6A4}
Beweisen Sie folgende Aussagen
\begin{enumerate}
\item[1.] Sei $(a_n)_{n\in\mathbb{N}}$ eine Folge mit $a_n\geq0$ und $\sum_{n=1}^\infty a_n<\infty$, dann folgt $\lim_{k\to\infty}\sum_{n=k}^\infty a_n=0$.
\item[2.] Sei $(a_n)_{n\in\mathbb{N}}$ eine monotone Folge und es gebe eine Teilfolge $(a_{n_k})_{k\in\mathbb{N}}$, sodass $a_{n_k}\to a$, dann folgt $a_n\to a$.
\end{enumerate}
\emph{Hinweis: Diese zwei Aussagen wurden im Beweis von Lemma 43 verwendet.
  Es ist sinnvoll, diesen nach dem Bearbeiten der Übungsaufgabe zu wiederholen.}

Zu Punkt 1, es gilt für alle $k\in\mathbb{N}$, dass $\sum_{n=k}^\infty a_n=\sum_{n=1}^\infty a_n-\sum_{n=1}^{k-1}a_n$.
Somit können wir, um zu zeigen, dass $\lim_{k\to\infty}\sum_{n=k}^\infty a_n=0$, genauso gut zeigen, dass $\lim_{k\to\infty}\sum_{n=1}^{k-1}a_n=\sum_{n=1}^\infty a_n$.
Dies gilt aber aufgrund der Eindeutigkeit des Grenzwertes $\sum_{n=1}^\infty a_n<\infty$.
\emph{Hier wird allerdings nicht klar, warum $a_n\geq0$ vorausgesetzt werden muss.}

Zu Punkt 2, sei $\varepsilon>0$ gegeben, dann existiert ein $k\in\mathbb{N}$, sodass $|a_{n_k}-a|<\varepsilon$.
Da die Folge monoton ist, gilt für alle $n\in\mathbb{N}$, dass $|a_{n+1}-a|<|a_{n}-a|$.
Hiermit gilt auch für alle $n\geq k$, dass $|a_n-a|<\varepsilon$, sodass $(a_n)\to a$.

Zu Lemma 43, hier wird behauptet, dass für unabhängige $X_1,X_2,\dots\in L^2$ mit  $\sum_{n=1}^\infty\operatorname{Var}(X_n)<\infty$ gilt $\sum_{k=1}^n(X_k-E[X_k])\xrightarrow{\text{f.s.}}0$.

Zum Beweis betrachten wir ohne Einschränkung zentrierte Zufallsvariablen, also $E[X_k]=0$, nutzen also quasi das Blockungslemma.
Wir greifen direkt auf die Maximal-Ungleichung aus dem Satz davor zurück und wenden sie auf die Folge $Y_i=X_{i+k}$ an.
Diese lautet für ein gegebenes $\varepsilon>0$ dann
\begin{align*}
  P\Bigl(\sup_{n\in \mathbb{N}}\Bigl|\sum_{i=1}^nY_i\Bigr|>\varepsilon\Bigr)
  &\leq\frac{1}{\varepsilon^2}\sum_{n=1}^\infty E[Y_n^2]
    \intertext{Setzt man $Y_i=X_{i+k}$ ein und betrachtet auf beiden Seiten den Limes $k\to\infty$, so ergibt sich mit $S_n=\sum_{i=1}^n S_i$}
    \lim_{k\to\infty}P\Bigl(\sup_{n\geq k}|S_n-S_k|>\varepsilon\Bigr)
  &\leq\lim_{k\to\infty}\frac{1}{\varepsilon^2}\sum_{n=k+1}^\infty E[X_n^2]\,.
    \intertext{Nun nutzen wir das, was wir in Punkt 1 bewiesen haben, angewendet auf die Folge $a_n=\operatorname{Var}(X_n)$.
    In der Tat ist laut Aufgabenstellung $\sum_{n=1}^\infty\operatorname{Var}(X_n)<\infty$.
    Aus Punkt 1 folgt $\lim_{k\to\infty}\sum_{n=k+1}^\infty a_n=0$.
    Eingesetzt in unsere Rechnung erhalten wir}
  &=0\,.
\end{align*}
Mit der Definition von stochastischer Konvergenz heißt das, die Folge $(Z_k)_k$ mit $Z_k=\sup_{n\geq k}|S_n-S_k|$ konvergiert stochastisch gegen 0.
Wo wir die unterschiedlichen Konvergenzen eingeführt haben (Definition 14), haben wir gleichzeitig besprochen, dass dies bedeutet, dass es eine Teilfolge $(k_i)_i$ gibt, sodass $(Z_{k_i})_i$ fast sicher gegen 0 konvergiert.
Wir schauen uns $Z_k$ an dieser Stelle genauer an.
Es gilt $Z_k=\sup_{n\geq k}\bigl|\sum_{i=k+1}^n X_i\bigr|$.
Die Folgenglieder haben also mit steigendem $k$ immer weniger Summanden, sodass die Folge fallend ist.
\emph{Damit diese Argumentation stimmt, müsste allerdings $X_i\geq 0$ vorausgesetzt werden.}
Es gilt also $P(A)=1$ mit $A=\{\omega\in\Omega\mid Z_{k_i}(\omega)\xrightarrow{i\to\infty}0\}$.
Nun benutzen wir das, was wir in Punkt 2 bewiesen haben, nämlich punktweise für $a_k=Z_k(\omega)$ für alle $\omega\in A$.
Da auf $A$ gilt, $a_{k_i}\xrightarrow{i\to\infty}0$ und $a_k$ auch fallend ist, gilt mit Punkt 2 für die gleichen $\omega\in A$, dass $Z_k(\omega)\xrightarrow{k\to\infty}0$.
Da wir vorhin gesagt haben, dass $P(A)=1$, ist somit auch $P\bigl(Z_k(\omega)\xrightarrow{k\to\infty}0\bigr)=1$.
Das heißt, $Z_k$ konvergiert also fast sicher gegen 0.
\emph{Hier fehlt noch eine letzte Überlegung, warum dann $\sum_{k=1}^n X_k\xrightarrow{n\to\infty}0$.}
\newpage


\bibliography{../../../books/wt}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% ispell-local-dictionary: "german"
%%% TeX-master: t
%%% End:
