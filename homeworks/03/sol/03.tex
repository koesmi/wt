\documentclass{article}
\usepackage[a4paper,margin=1.875in,top=1.1in,bottom=1.1in]{geometry}

\usepackage{amsmath,mathtools,bbm,amssymb}
\usepackage[german]{babel}

\usepackage{setspace}
\doublespacing

\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{0pt} 
\pagestyle{fancy}
\lhead{Blatt 3 Nicolas und Evgenij}\rhead{Seite \thepage}
\fancyfoot{}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,arrows.meta}

\usepackage[numbers,round]{natbib}
\bibliographystyle{alphadin}
\usepackage{url}
\usepackage{hyperref}

\begin{document}

\paragraph{B3A1}
% eventuell hilfreich https://math.stackexchange.com/questions/1904411/almost-sure-convergence-and-lim-sup
Bei Teilaufgabe (a) möchten wir zwei Äquivalenzen zeigen.
Zunächst einmal, dass $X_n\xrightarrow{\text{f.s.}}X$ äquivalent dazu ist, dass für alle $\varepsilon>0$ gilt, $\lim_n\bigl(\bigcup_{m=n}^\infty\{|X_m-X|>\varepsilon\}\bigr)=0$.
Als zweites möchten wir zeigen, dass dies wieder äquivalent dazu ist, dass $\sup_{m\geq n}|X_n-X|\xrightarrow{P}0$.
Hierfür gehen wir wie im Beweis von Satz 6.1.2 in \cite{hesse} vor.
Die Idee ist, die Definition der fast sicheren Konvergenz in Form von Vereinigungen und schnitten zu umschreiben.
Da $P$ ein Wahrscheinlichkeitsmaß ist, können wir den Limes, der bei der fast sicheren Konvergenz im Argument von $P$ steht, rausziehen.

Gilt $X_n\xrightarrow{\text{f.s.}}X$, so heißt es nach Defintion, dass $P(\lim|X_n-X|=0)=1$, oder
\begin{align*}
0
  &=1-P(\lim|X_n-X|=0)
    \intertext{Mit der Definition von Konvergenz bedeutet das für alle $\varepsilon>0$}
  &=1-P(\exists n\in\mathbb{N}\,\forall m\geq n~|X_m-X|<\varepsilon)\,.
    \intertext{$\omega\in\{\exists n\in\mathbb{N}\,\forall m\geq n~|X_m-X|<\varepsilon\}$ gilt genau dann, wenn $\omega$ in irgendeiner der Mengen $\{\forall m\geq1|X_m-X|<\varepsilon\},\{\forall m\geq2|X_m-X|<\varepsilon\},\dots$ ist.
    Entsprechend gilt $\omega\in\{\forall m\geq n~|X_m-X|<\varepsilon\}$ genau dann, wenn $\omega$ in all den Mengen $\{|X_n-X|<\varepsilon\},\{|X_{n+1}-X|<\varepsilon\},\dots$ vorkommt.
    Hierdurch lässt sich umschreiben}
  &=1-P\Bigl(\bigcup\nolimits_{n=1}^\infty\bigcap\nolimits_{m=n}^\infty\{|X_m-X|<\varepsilon\}\Bigr)\,.
    \intertext{Dadurch, dass $\{|X_m-X|<\varepsilon\}^\mathrm{c}=\{|X_m-X|\geq\varepsilon\}$, sowie $\bigl(\bigcup A_i\bigr)^\mathrm{c}=\bigcap A_i^\mathrm{c}$ und $\bigl(\bigcap A_i\bigr)^\mathrm{c}=\bigcup A_i^\mathrm{c}$ folgt}
  &=P\Bigl(\bigcap\nolimits_{n=1}^\infty\bigcup\nolimits_{m=n}^\infty\{|X_m-X|\geq\varepsilon\}\Bigr)\,.
    \intertext{Hierbei konvergiert die Folge $\bigl(\bigcup_{m=n}^\infty\{|X_m-X|\geq\varepsilon\}\bigr)_n$ von oben gegen $\bigcap\nolimits_{n=1}^\infty\bigcup\nolimits_{m=n}^\infty\{|X_m-X|\geq\varepsilon\}$, sodass wir nach Satz A.14 den Limes herausziehen können und sich ergibt}
  &=\lim\nolimits_{n}P\Bigl(\bigcup\nolimits_{m=n}^\infty\{|X_m-X|\geq\varepsilon\}\Bigr)\,,
\end{align*}
womit wir schon mal die erste Äquivalenz gezeigt haben.

Zur zweiten Äqui\-va\-lenz machen wir wieder die Über\-le\-gung mit den Quantoren.
Es gilt $\omega\in\bigcup_{m=n}^\infty\{|X_m-X|\geq\varepsilon\}$ genau dann, wenn $\omega$ in mindestens einer der Mengen $\{|X_n-X|\geq\varepsilon\},\{|X_{n+1}-X|\geq\varepsilon\},\dots$ liegt, also in einer Menge $\{|X_k-X|\geq\varepsilon\}$ mit $k\geq n$ so, dass für alle $m\geq n$ gilt $|X_k-X|\geq|X_m-X|$ und eventuell noch in weiteren $\{|X_m-X|\geq\varepsilon\}$, wobei $|X_m-X|\leq |X_k-X|$.
Also genau dann, wenn $\omega\in\{\sup_{m\geq n}|X_m-X|\geq\varepsilon\}$.
Es folgt
    $\lim\nolimits_{n}P\bigl(\bigcup\nolimits_{m=n}^\infty\{|X_m-X|\geq\varepsilon\}\bigr)
  =\lim\nolimits_n P(\sup\nolimits_{m\geq n}|X_m-X|\geq\varepsilon)\,.$
Da die Beziehung für beliebige $\varepsilon>0$ gilt, ist die gesuchte Äquivalenz mit der Definition der stochastischen Konvergenz 14.\emph{ii} gezeigt.

Zur Teilaufgabe (b) bemerken wir, dass, wenn $(X_n)$ fast sicher gegen ein $X$ konvergiert, dann auch $|X_n-X|\wedge1$ fast sicher gegen 0 konvergiert.
Da $|X_n-X|\wedge1$ die 1 als integrierbare Majorante hat,  konvergiert mit Theorem 14 über majorisierte Konvergenz $E[|X_n-X|\wedge 1]$ fast sicher gegen 0 und schließlich $(X_n)$ nach Lemma 17 stochastisch gegen $X$.

Als alternative Lösung gehen wie im Beweis zu Satz 6.2.2 in \cite{hesse} vor.
Nach Teilaufgabe (a) gilt $\lim_nP\bigl(\bigcup\nolimits_{m=n}^\infty\{|X_m-X|\geq\varepsilon\}\bigr)=0$.
Für alle $n\in\mathbb{N}$ gilt die Inklusion $\{|X_n-X|\geq\varepsilon\}\subseteq\bigcup\nolimits_{m=n}^\infty\{|X_m-X|\geq\varepsilon\}$.
Aufgrund der Monotonie von $P$ gilt somit auch $\lim_{n}P(|X_n-X|\geq\varepsilon)=0$, also  $X_n\xrightarrow{P}X$.

Bei Teilaufgabe (c) wollen wir zeigen, dass $(X_n)$ genau dann fast sicher konvergiert, wenn gilt, dass $\lim_n P\bigl(\bigcup_{m=1}^\infty\{|X_{m+n}-X_n|\geq\varepsilon\}\bigr)=0$.
Wir gehen wie in Satz 2.3.3.3 aus \cite{rueschendorf} vor.
Sei $(X_n)$ also fast sicher konvergent.
Insbesondere ist es, außer in einer $P$-Nullmenge $N\subset\Omega$, punktweise konvergent.
Das heißt für alle $\omega\in\Omega\setminus N$ sind $\bigl(X_n(\omega)\bigr)_n$ Cauchy-Folgen in den reellen Zahlen und das ist äquivalent dazu, dass $(X_n)$ fast sicher eine Cauchy-Folge ist.
Entsprechend der Argumentation mit Quantoren aus Teilaufgabe (a) heißt das, für alle $\varepsilon>0$ gilt
\begin{align*}
  1
  &=P\bigl(\bigcup\nolimits_{n=1}^\infty\bigcap\nolimits_{m=n}^\infty\{|X_m-X_n|<\varepsilon\}\bigr)\,.
    \intertext{Mit Stetigkeit von unten folgt}
  &=\lim\nolimits_nP\Bigl(\bigcap\nolimits_{m=n}^\infty\{|X_m-X_n|<\varepsilon\}\Bigr)\,.
\end{align*}
Da $\bigl(\bigcap A_n\bigr)^\mathrm{c}=\bigcup A_n^\mathrm{c}$ folgt mit Verschieben $m\mapsto m-n$ die Behauptung.
\newpage
\paragraph{B3A2}
Bei Teilaufgabe (a) ist zu zeigen, dass wenn $X_n\leq Y_n\leq Z_n$ für alle $n\in\mathbb{N}$ und $X_n\xrightarrow{P}X$, $Y_n\xrightarrow{P}Y$ sowie $Z_n\xrightarrow{P}Z$, dann $X_n+Y_n\xrightarrow{P}X+Y$. Es ist also zu zeigen, für alle $\varepsilon>0$ gilt $\lim_nP(|X_n-X+Y_n-Y|\geq\varepsilon)=0$.
Für den Beweis gehen wir entsprechend \cite{tsitsiklis} vor.
Wir möchten uns zunächst Folgen $(a_n)$ und $(b_n)$ in den reellen Zahlen anschauen und zeigen, dass wenn $a_n\to a$ und $b_n\to b$ gilt, dann auch $a_n+b_n\to a+b$ gilt.
Da $\bigl(P(|X_n-X|\geq\varepsilon)\bigr)_n$ und $\bigl(P(|Y_n-Y|\geq\varepsilon)\bigr)_n$ Folgen in reellen Zahlen sind, können wir die Summenregel dann auf diese anwenden.

$a_n\to a$ bedeutet, dass für alle $\varepsilon>0$ ein $n_0\in\mathbb{N}$ existiert, sodass für alle $n\geq n_0$ gilt $|a_n-a|<\varepsilon$.
Sei nun $\varepsilon>0$ beliebig vorgegeben und $n_0\in\mathbb{N}$ so, dass $|a_n-a|<\frac{\varepsilon}{2}$.
Weiterhin sei $n_0'\in\mathbb{N}$ so, dass $|b_n-b|<\frac{\varepsilon}{2}$.
Wenn wir nun ein $n\geq n_0\vee n_0'$ wählen, so gilt nach Dreiecksungleichung $|a_n-a+b_n-b|\leq|a_n-a|+|b_n-b|<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon$, sodass $a_n+b_n\to a+b$.

Nun möchten wir die entsprechende Aussage für stochastische Konvergenz zeigen.
Das machen wir so ähnlich wie im Beweis von Lemma 16.
Wir schätzen die Wahrscheinlichkeit, $P(|X_n-X+Y_n-Y|\geq\varepsilon)$, die im Limes verschwinden soll, nach oben hin mithilfe der Dreiecksungleichung durch die Einzelwahrscheinlichkeiten $P(|X_n-X|\geq\varepsilon)$ und $P(|Y_n-Y|\geq\varepsilon)$ ab, von denen wir wissen, dass sie im Limes verschwinden.

Sei hierfür wieder $\varepsilon>0$ beliebig.
In Analogie zur Konvergenz der reellen Folgen $(a_n)$ und $(b_n)$ gilt mit der Dreiecksungleichung und der Monotonie von $P$, dass
\begin{align*}
  P(|X_n-X+Y_n-Y|\geq2\varepsilon)
  &\leq P(|X_n-X|+|Y_n-Y|\geq2\varepsilon)\,.
    \intertext{Nun ist die Wahrscheinlichkeit, dass $|X_n-X|+|Y_n-Y|\geq\varepsilon$ kleiner als die Wahrscheinlichkeit dafür, dass nur $|X_n-X|\geq\varepsilon$ oder nur $|Y_n-Y|\geq\varepsilon$.
    Damit gilt}
  &\leq P\bigl(\{|X_n-X|\geq\varepsilon\}\cup\{|Y_n-Y|\geq\varepsilon\}\bigr)\,.
    \intertext{Da $P$ subadditiv ist, können wir abschätzen}
  &\leq P\bigl(\{|X_n-X|\geq\varepsilon\})+P(\{|Y_n-Y|\geq\varepsilon\}\bigr)\,.
\end{align*}
Da $X_n\xrightarrow{P}X$ und $Y_n\xrightarrow{P}Y$, sind die beiden Terme in der obigen Summe Folgen in $\mathbb{R}$, die gegen 0 konvergieren.
Wir haben vorhin auch erklärt, dass dann die Summe der Folgen gegen 0 konvergiert.
Damit ist die reelle Folge $P(|X_n-X+Y_n-Y|\geq\varepsilon)$ mit etwas nach oben abgeschätzt, dass für $n\to\infty$ gegen 0 konvergiert, sodass $\lim_nP(|X_n-X+Y_n-Y|\geq\varepsilon)=0$ und $X_n+Y_n\xrightarrow{P}X+Y$.
    
Bei Teilaufgabe (b) konvergiert nun zusätzlich $E[X_n]\to E[X]$ und $E[Z_n]\to E[Z]$ und wir sollen zeigen, dass $E[Y_n]\to E[Y]$, also $Y_n\xrightarrow{\mathcal{L}^p}Y$.
Entsprechend Tipp wollen wir Theorem 22 verwenden.
Da laut Aufgabenstellung bereits $Y_n\xrightarrow{P}Y$ gilt, zeigen wir, dass $(Y_n)$ gleichgradig integrierbar ist.
Dann gilt nach Theorem 22, dass $Y_n\xrightarrow{\mathcal{L}^p}Y$.
Um die gleichgradige Integrierbarkeit von $(Y_n)$ zu zeigen, reicht es nach Lemma 20 zu zeigen, dass $E[|Y_n|]<\infty$ und dass $\lim_{\varepsilon\to0}\sup_{A:P(A)<\varepsilon}\sup_{n}E[|Y_n|\mathbbm{1}_A]=0$.
Da $X_n\leq Y_n\leq Z_n$ ist $|Y_n|\leq|X_n|+|Z_n|$ für alle $n$.
Wegen der Monotonie und Linearität des Erwartungswertes gilt für den Erwartungswert $E[|Y_n|]$, dass $E[|Y_n|]\leq E[|X_n|+|Z_n|]=E[|X_n|]+E[|Z_n|]<\infty$, da die Folgen $(E[X_n])$ und $(E[Z_n])$ konvergent und somit beschränkt sind.

Nun ist noch zu zeigen, dass $(Y_n)$ den zweiten Part der Bedingung \emph{(ii)} von Lemma 20 erfüllt, also, dass $\lim_{\varepsilon\to0}\sup_{A:P(A)<\varepsilon}\sup_{n}E[|Y_n|\mathbbm{1}_A]=0$ gilt.
Wieder gilt wegen der Monotonie des Erwartungswertes
\begin{align*}
  \lim_{\varepsilon\to0}\sup_{A:P(A)<\varepsilon}\sup_{n\in\mathbb{N}}E[|Y_n|\mathbbm{1}_A]
  &\leq\lim_{\varepsilon\to0}\sup_{A:P(A)<\varepsilon}\sup_{n\in\mathbb{N}}E[(|X_n|+|Z_n|)\mathbbm{1}_A]\,.
      \intertext{Wegen der Linearität des Erwartungswertes gilt}
  &=\lim_{\varepsilon\to0}\sup_{A:P(A)<\varepsilon}\sup_{n\in\mathbb{N}}E[|X_n|\mathbbm{1}_A]\\
  &\quad+\lim_{\varepsilon\to0}\sup_{A:P(A)<\varepsilon}\sup_{n\in\mathbb{N}}E[|Z_n|\mathbbm{1}_A]=0\,,
\end{align*}
da $(X_n)$ und $(Z_n)$ nach Theorem 22 gleichgradig integrierbar sind und somit Bedingung \emph{(ii)} von Lemma 20  erfüllen.

Somit erfüllt $(Y_n)$ Bedingung \emph{(ii)} von Lemma 20 und ist damit gleichgradig integrierbar.
Da $Y\xrightarrow{P}Y$ folgt mit Theorem 22, dass $E[Y_n]\to E[Y]$.
\newpage
\paragraph{B3A3}
Wir haben hier $K_n=\prod_{i=1}^nY_i$ mit $P\bigl(Y_i=\frac{5}{3}\bigr)=P\bigl(Y_i=\frac{1}{2}\bigr)=\frac{1}{2}$ gegeben und sollen bei Aufgabe (a) $EK_n$ bestimmen sowie zeigen, dass $\lim_n EK_n=\infty$.
Nach der Definition der $K_n$ gilt
\begin{align*}
  E[K_n]
  &=E\Bigl[\prod\nolimits_{i=1}^n Y_i\Bigr]\,.
    \intertext{Da die $Y_i$ stochastisch unabhängig sind, erhalten wir nach Satz 27}
  &=\Bigl(\frac{5}{3}\cdot\frac{1}{2}+\frac{1}{2}\cdot\frac{1}{2}\Bigr)^n=\Bigl(\frac{13}{12}\Bigr)^n\,.
\end{align*}
Da $EK_{n+1}>EK_n$ gilt $\lim_{n\to\infty}EK_n=\infty$.

Bei der Teilaufgabe (b) sollen wir zeigen, dass $K_n$ dennoch stochastisch gegen 0 konvergiert.
Hierfür nutzen wir den Tipp und betrachten $\log K_n=\sum^n\log Y_i$ und wenden das schwache Gesetz der großen Zahlen auf die $\log Y_i$ an.
$(\log Y_i)$ genügt dem schwachen Gesetz der großen Zahlen, wenn die $\log Y_i$ unabhängig identisch verteilt sind und ihr Erwartungswert sowie ihre Varianz endlich sind.
Da die $Y_i$ unabhängig identisch verteilt sind, sind es die $\log Y_i$ auch.
Wir rechnen nach, dass $E[\log Y_1]=\frac{1}{2}\log\frac{5}{3}+\frac{1}{2}\log\frac{1}{2}<0$ und $|E[\log Y_i]|<\infty$.
$E[(\log Y_i)^2]=\frac{1}{2}\bigl(\log\frac{5}{3}\bigr)^2+\frac{1}{2}\bigl(\log\frac{1}{2}\bigr)^2<\infty$, sodass Erwartungswert und Varianz endlich sind.
Somit genügt $(\log Y_i)_i$ dem schwachen Gesetz der großen Zahlen, sodass gilt $\frac{1}{n}\sum^n\log Y_i\xrightarrow{P}E[\log Y_1]<0$, also $\frac{\log K_n}{n}\xrightarrow{P}E[\log Y_1]<0$.
Da für den Nenner von $\frac{\log K_n}{n}$ gilt $n\to\infty$ und $E[\log Y_1]<0$, muss gelten $\log K_n\xrightarrow{P}-\infty$, also $K_n\xrightarrow{P}\mathrm{e}^{-\infty}=0$.
% https://math.stackexchange.com/questions/1938662/using-the-law-of-large-numbers-to-show-that-m-n-prod-k-1n-1x-k-converge

\newpage
\paragraph{B3A4}
Sei $(X_n)_{n\in\mathbb{N}}$ eine Folge stochastisch unabhängiger Zufallsvariablen mit $P(X_n=\sqrt{n})=\frac{1}{n}=1-P(X_n=0)$.
Wir sollen diese auf stochastische, $P$-fast-sichere und $L^p$-Konvergenz für alle $p\geq1$ untersuchen.
Weiterhin ist gefragt, ob $(X_n)$ gleichgradig integrierbar ist.

Wir fragen uns, ob $(X_n)$ stochastisch konvergiert, also ob für alle $\varepsilon>0$ gilt $\lim P(|X_n-X|\geq\varepsilon)=0$.
Wir vermuten, dass, wenn $(X_n)$ konvergiert, es gegen $X=0$ konvergiert.
Die Frage ist also, ob für alle $\varepsilon>0$ gilt $\lim P(X_n\geq\varepsilon)=0$.
Sei, um das zu klären, ein $\varepsilon>0$ gegeben.
Wenn $X_n\geq\varepsilon$, dann gilt, weil $X_n$ nach $\{0,\sqrt{n}\}$ abbildet, $X_n=\sqrt{n}$.
Somit ist der Limes gegeben durch $\lim P(X_n\geq\varepsilon)=P(X_n=\sqrt{n})=\lim\frac{1}{n}=0$.
$(X_n)$ konvergiert also stochastisch gegen $X=0$.

Wir fragen uns nun, ob $(X_n)$ $P$-fast sicher konvergiert, das heißt also, ob $P(\lim |X_n-X|=0)=1$.
Um das zu klären folgen wir Beispiel 6.7 in \cite{hesse}.
Wir bemerken, dass, wenn $X_n\xrightarrow{\text{f.s.}}X$, nach Aufgabe 1 (b) auch $X_n\xrightarrow{P}X$.
Wegen Lemma 16 über die Eindeutigkeit der Grenzwerte der stochastischen Konvergenz muss $X=0$ gelten.
Das heißt also, wenn $(X_n)\xrightarrow{\text{f.s.}}X$, dann ist $X=0$.
Wir fragen uns also, ob $P(\lim X_n=0)=1$.
Nach Aufgabe 1 können wir uns auch genauso gut fragen, ob für alle $\varepsilon>0$ gilt $\lim_nP\bigl(\bigcup_{m=n}^\infty\{X_m\geq\varepsilon\}\bigr)=0$.
Da $\bigl(\bigcup A_n\bigr)^\mathrm{c}=\bigcap A_n^\mathrm{c}$ und für alle $m$ gilt $\{X_m\geq\varepsilon\}=\{X_m<\varepsilon\}^\mathrm{c}$, können wir für alle $n\in\mathbb{N}$ schreiben $P\bigl(\bigcup\nolimits_{m=n}^\infty\{X_m\geq\varepsilon\}\bigr)=1-P\bigl(\bigcap\nolimits_{m=n}^\infty\{X_m<\varepsilon\}\bigr)$.
$P$ ist als Wahrscheinlichkeitsmaß endlich und somit nach Satz A.14 stetig von oben.
Wir können somit schreiben
\begin{align*}
  P\Bigl(\bigcap\nolimits_{m=n}^\infty\{X_m<\varepsilon\}\Bigr)
  &=\lim_{N\to\infty}P\Bigl(\bigcap\nolimits_{m=n}^N\{X_m<\varepsilon\}\Bigr)\,.
  \intertext{Sei nun $n$ so gewählt, dass $\sqrt{n}\geq\varepsilon$, also zum Beispiel $n=\bigl\lceil\varepsilon^2\bigr\rceil$.
    Dann gilt für $m\geq n$, dass $\{X_m<\varepsilon\}=\{X_m=0\}$, also}
  &=\lim_{N\to\infty}P\Bigl(\bigcap\nolimits_{m=n}^N\{X_m=0\}\Bigr)\,.
    \intertext{Da die $X_m$ unabhängig sind, gilt}
  &=\lim_{N\to\infty}\prod\nolimits_{m=n}^N\frac{m-1}{m}\,.
    \intertext{Da gilt $\prod_{m=n}^{N+1}\frac{m-1}{m}<\prod_{m=n}^N\frac{m-1}{m}$ erhalten wir}
  &=0
\end{align*}
und insgesamt somit $\lim_nP\bigl(\bigcup_{m=n}^\infty\{X_m\geq\varepsilon\}\bigr)=1$.
Nach Aufgabe 1 konvergiert $(X_n)$ also nicht fast sicher.

Wir überlegen uns noch, ob $X_n\xrightarrow{L^p}X$ für $p\geq1$ und folgen hier Beispiel 6.12 aus \cite{hesse}.
Für den Erwartungswert von $E[|X_n|^p]$ ergibt sich $E[|X_n|^p]=\sqrt{n}^p\cdot\frac{1}{n}+0\cdot\frac{n-1}{n}=n^{\frac{p}{2}-1}$.
Nach Theorem 22 ist $X=0$, sollte $X_n\xrightarrow{L^p}X$ gelten.
Somit konvergiert $E[|X_n|^p]$, falls $\frac{p}{2}-1<0$, also $p<2$.
Bei uns ist aber $p\geq1$, sodass $(X_n)$ für $1\leq p<2$ bezüglich der $L^p$-Norm konvergiert.

Da die Folge $(X_n)$ also bezüglich der $L^1$-Norm konvergiert, ist sie nach Theorem 22 auch gleichgradig integrierbar.
\newpage
\bibliography{../../../books/wt}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% ispell-local-dictionary: "german"
%%% TeX-master: t
%%% End:
