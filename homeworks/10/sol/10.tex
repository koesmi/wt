\documentclass{article}
\usepackage[a4paper,margin=1.875in,top=1.5in,bottom=1.5in]{geometry}

\usepackage{amsmath,mathtools,bbm,amssymb}
\usepackage[german]{babel}

\usepackage{setspace}
\doublespacing

\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{0pt} 
\pagestyle{fancy}
\lhead{Blatt 10 Nicolas und Evgenij}\rhead{Seite \thepage}
\fancyfoot{}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,arrows.meta}

\usepackage[numbers]{natbib}
\bibliographystyle{alphadin}
\usepackage{url}
\usepackage{hyperref}

\begin{document}

\paragraph{Aufgabe 1 \textnormal{(4 Punkte)}.}
Seien $Y$ und $Y_n$ für alle $n\in\mathbb{N}$ Zufallsvariablen mit Werten in $\mathbb{Z}$.
Zeigen Sie
\[
Y_n\Rightarrow Y\quad\Longleftrightarrow\quad\forall j\in\mathbb{Z}\colon P(Y_n=j)\xrightarrow{n\to\infty}P(Y=j).
\]
Wir verwenden die Rücktransformation der charakteristischen Funktionen $\varphi_n$ und $\varphi$ von $Y_n$ und $Y$.
Für $\varphi$ gilt
\[
  P(X=j)=\frac{1}{2\pi}\int_{-\pi}^\pi\mathrm{e}^{-\mathrm{i}tj}\varphi(t)\mathrm{d}t\,.
\]
In der Tat gilt mit der Definition der charakteristischen Funktion
\begin{align*}
  \frac{1}{2\pi}\int_{-\pi}^\pi\mathrm{e}^{-\mathrm{i}tj}\varphi(t)\mathrm{d}t
  &=\frac{1}{2\pi}\int_{-\pi}^\pi\mathrm{e}^{-\mathrm{i}jt}E_k\bigl[\mathrm{e}^{\mathrm{i}kt}\bigr]\mathrm{d}t\,.
    \intertext{Da $E_k$ nur Masse bei $X=k$ hat erhalten wir}
  &=\frac{1}{2\pi}\int_{-\pi}^\pi\mathrm{e}^{-\mathrm{i}jt}\sum_{k\in\mathbb{Z}}\mathrm{e}^{\mathrm{i}kt}P(X=k)\mathrm{d}t
    \intertext{Zusammenfassen liefert, \emph{wobei man sich eventuell überlegen sollte, ob man die Summe und das Integral vertauschen darf}}
  &=\sum_{k\in\mathbb{Z}}P(X=k)\frac{1}{2\pi}\int_{-\pi}^\pi\mathrm{e}^{\mathrm{i}(k-j)t}\mathrm{d}t\,.
\end{align*}
Wenn $k-j=0$, so ist der Integrand 1.
Wenn $k-j=\ell\neq0$, gilt
\begin{align*}
  \int_{-\pi}^\pi\mathrm{e}^{\mathrm{i}\ell t}\mathrm{d}t
  &=\frac{1}{\mathrm{i}\ell}\bigl(\mathrm{e}^{\mathrm{i}\ell\pi}-\mathrm{e}^{-\mathrm{i}\ell\pi}\bigr)=\frac{1}{\mathrm{i}\ell}\bigl((\pm1)-(\pm1)\bigr)=0\,.
\end{align*}
Eingesetzt in die obige Rechnung verschwinden somit alle Summanden mit $k\neq j$ und wir erhalten die Darstellung für die Rücktransformation der charakteristischen Funktion.
Entsprechendes gilt dann auch für $Y_n$ und $\varphi_n$.
Mithilfe dieser Rücktransformationen erhalten wir die gesuchte Konvergenz.
Für den Abstand von $P(Y_n=j)$ und $P(Y=j)$ gilt mithilfe Rücktransformation
\begin{align*}
  |P(Y_n=j)-P(Y=j)|
  &=\left|\frac{1}{2\pi}\int_{-\pi}^\pi\mathrm{e}^{-\mathrm{i}tj}\varphi_n\mathrm{d}t-\frac{1}{2\pi}\int_{-\pi}^\pi\mathrm{e}^{-\mathrm{i}tj}\varphi\mathrm{d}t\right|\,.
    \intertext{Mit Zusammenfassen und der \glqq Dreiecksungleichung\grqq{} des Integrals können wir abschätzen}
  &\leq\frac{1}{2\pi}\int_{-\pi}^\pi\bigl|\mathrm{e}^{\mathrm{i}tj}\bigl(\varphi_n(t)-\varphi(t)\bigr)\bigr|\mathrm{d}t\,.
    \intertext{Das vereinfacht sich, weil $\bigl|\mathrm{e}^{\mathrm{i}tj}\bigr|=1$.
    Weiterhin konvergiert $\varphi_n(t)$ gegen $\varphi(t)$ nach dem Portmanteau-Theorem, denn $\mathrm{e}^{\mathrm{i}tj}$ ist Lipschitz-stetig.
    Da $|\varphi_n(t)|\leq1$ können wir schließlich den Satz über majorisierte Konvergenz verwenden und erhalten}
  &\xrightarrow{n\to\infty}0\,.
\end{align*}

Wenn andererseits für alle $j\in\mathbb{Z}$ gilt, dass $P(Y_n=j)\xrightarrow{n\to\infty}P(Y=j)$, dann gilt auch $\sum_{j\in\mathbb{Z}}f(j)P(Y_n=j)\xrightarrow{n\to\infty}\sum_{j\in\mathbb{Z}}f(j)P(Y=j)$ für alle $f\in{\cal C}_\mathrm{b}$ und somit $Y_n\Rightarrow Y$.
\newpage

\paragraph{Aufgabe 2 \textnormal{(4 Punkte)}.}
Zeigen Sie, dass jedes Wahrscheinlichkeitsmaß auf $\mathbb{R}$ schwacher Limes einer Folge von diskreten Wahrscheinlichkeitsmaßen ist.

\emph{Sei hierfür ein Wahrscheinlichkeitsmaß $P\in{\cal P}_1(\mathbb{R})$ und ein $f\in{\cal C}_\mathrm{b}(\mathbb{R})$ gegeben.
Dann gilt
\[
\int f\mathrm{d}P=\sup\left\{\int g\mathrm{d}P~\middle\vert~ g\leq f\text{ einfach}\right\}\,
\]
wobei einfach heißt, dass Folgen $(\alpha_n)$ in $\mathbb{R}$ und $(A_n)\in{\cal B}(\mathbb{R})$ existieren, sodass $g=\sum\alpha_n \mathbbm{1}_{A_n}$ und $\int g\mathrm{d}P=\sum\alpha_n P(A_n)$.
Gebe es eine Folge diskreter Wahrscheinlichkeitsmaße $(P_n)$, sodass $P$ schwacher Limes von $(P_n)$ ist, dann wäre $\int f\mathrm{d}P_n=\sum_{j\in\mathbb{Z}}f(j)P_n(X=j)$.
Eventuell kann man auch die Konstruktion $\frac{1}{n}\sum\delta_{k/n}$ aus Aufgabe 1 von Blatt 9 verwenden.
Außerdem gibt es die einfache Funktion $S_J:=\sum_{j=-J}^Jf(j)\mathbbm{1}_{X^{-1}\{j\}}$, die $g$ approximiert, vergleiche \cite{217980}.}
% https://math.stackexchange.com/questions/217974/equivalence-of-lebesgue-expectation-to-discrete-expectation-for-discrete-random
\newpage

\paragraph{Aufgabe 3 \textnormal{(4 Punkte)}.} Sei $(\alpha_n)_{n\in\mathbb{N}}$ eine Folge mit $\alpha_n\in(0,\infty)$.
Weiter sei $(X_n)_{n\in\mathbb{N}}$ eine Folge von Zufallsvariablen, sodass $X_n$ exponentialverteilt mit Parameter $\alpha_n$ ist, das heißt, $X_n$ besitzt die Dichte
\[
  f_n(x)=\mathbbm{1}_{\{x\geq0\}}\alpha_n\mathrm{e}^{-\alpha_n x}\,.
\]
Zeigen Sie die schwache Konvergenz von $(X_n)_{n\in\mathbb{N}}$ für $n\to\infty$, falls $\alpha_n\to\infty$ für $n\to\infty$.

Wir wollen den Satz von Lévy verwenden.
Demnach konvergiert $X_n$ in Verteilung gegen eine Zufallsvariable $X$, wenn $\varphi_n\xrightarrow{n\to\infty}\varphi$.
Es gilt $\varphi_n(t)=\frac{\alpha_n}{\alpha_n-\mathrm{i}t}$ und mit dem Satz von de L'Hospital $\lim_{n\to\infty}\frac{\alpha_n}{\alpha_n-\mathrm{i}t}=1$.
Da 1 stetig in 0 ist, konvergiert also auch $(X_n)$.

\paragraph{Aufgabe 4 \textnormal{(4 Punkte)}.}
Zeigen Sie, dass es einen Homöomorphismus zwischen $\mathbb{R}$ und den Dirac Maßen auf $\mathbb{R}$ mit der schwachen Konvergenz gibt.

Es gilt,
\[
\delta_x(A)=
\begin{cases}
  1,&x\in A\,,\\
  0,&x\notin A\,.
\end{cases}
\]
Die Topologie auf den Dirac Maßen ist die gröbste Topologie, sodass für alle $f\in{\cal C}_\mathrm{b}(\mathbb{R})$ die Abbildung $\delta_x\mapsto\int f\delta_x(\mathrm{d}y)=f(x)$ stetig ist.
Wir nehmen an, dass der Homöomorphismus $x\mapsto\delta_x$ ist.
Da $x$ der einzige Parameter von $\delta_x$ ist, ist dieser schon mal bijektiv.

Die Prohorov-Metrik $d_\mathrm{P}$ ist in Bemerkung 13.14.iii in \cite{klenke} definiert.
Sei $d'_\mathrm{P}(\mu,\nu):=\inf\{\varepsilon>0:\mu(B)\leq\nu(B^\varepsilon)+\varepsilon\text{ für jedes }B\in{\cal B}(E)\}$.
Dann ist $d_\mathrm{P}(\mu,\nu):=\max\{d'_\mathrm{P}(\mu,\nu),d'_\mathrm{P}(\nu,\mu)\}$.

In \cite{4438264} steht, $d_\mathrm{P}(\delta_x,\delta_y)=d(x,y)\wedge1$.
\emph{Das sollte noch nachgerechnet werden.}

Wir zeigen noch, dass $x\mapsto\delta_x$ und $\delta_x\mapsto x$ stetig sind, also, dass Bilder und Urbilder offener Mengen offen sind.
Sei $U\subset\mathbb{R}$ offen.
Sei $\delta_x$ in $\delta_U$ gegeben.
Sei $\varepsilon>0$ so, dass $B_\varepsilon(x)\subset U$.
Dann gilt für ein $\delta_y\in \delta_U$, dass $|x-y|<\varepsilon$, also auch $d_\mathrm{P}(\delta_x,\delta_y)=|x-y|\wedge1<\varepsilon$.
Somit ist $\delta_U$ offen.
Sei nun andererseits $\delta_U$ offen.
Sei $x\in U$ gegeben.
Sei $0<\varepsilon<1$ so, dass $B_\varepsilon\bigl(\delta(x)\bigr)\subset \delta_U$.
Dann gilt für ein $y\in U$, dass $d_\mathrm{P}(x,y)=|x-y|\wedge1<\varepsilon$, also auch, dass $|x-y|<\varepsilon$, denn, $\varepsilon<1$.
Das heißt, $B_\varepsilon(x)\subset U$, also ist $U$ offen.

\bibliography{../../../books/wt}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% ispell-local-dictionary: "german"
%%% TeX-master: t
%%% End:
