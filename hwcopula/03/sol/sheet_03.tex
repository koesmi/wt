\documentclass{article}
\usepackage[a4paper,margin=1.875in,top=1.875in,bottom=1.875in]{geometry}
\usepackage{amsmath,mathtools,bbm,amssymb,stmaryrd,wasysym}
\usepackage{mathrsfs}

\usepackage{setspace}
\doublespacing

\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{0pt} 
\pagestyle{fancy}
\lhead{Sheet 3 Evgenij}\rhead{Page \thepage}
\fancyfoot{}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,arrows}

\usepackage[numbers]{natbib}
\bibliographystyle{alphadin}
\usepackage{url}
\usepackage{hyperref}

\begin{document}

\paragraph{Exercise 1 \textnormal{(4 Points)}.}
The Clayton Copula with parameter $\theta>0$ is given by
\[
C_\theta(u,v)=(u^{-\theta}+v^{-\theta}-1)^{-1/\theta}\,.
\]
Show that the Clayton copula is an archimedian copula.

We need to find a 2-monotone or convex $\varphi$ so that $(u^{-\theta}+v^{-\theta}-1)^{-1/\theta}=\phi(\varphi^{-1}(u)+\varphi^{-1}(v))$.
If we set the pseudo-inverse of the generator to $\varphi^{-1}(t)=t^{-\theta}-1$ for $t\in(0,1]$ and $t_0$ for $t=0$, then we get that $\varphi^{-1}(C_\theta(u,v))=u^{-\theta}+v^{-\theta}-2=\varphi^{-1}(u)+\varphi^{-1}(v)$.
That is, the Clayton copula is an archimedian copula with generator $\varphi(t)=(t+1)^{-1/\theta}$.
\pagebreak
\paragraph{Exercise 3 \textnormal{(4 Points)}.}
Prove Proposition 3.4:
Let $X=(X_1,\dots,X_d)$ and $Y=(Y_1,\dots,Y_d)$ be $d$-dimensional random vectors.
Then

\begin{itemize}
\item [(iv)] $X\leq_{sm}Y\quad\implies\quad X\leq_{dcx}Y\implies\sum_{i=1}^dX_i\leq_{cx}\sum_{i=1}^dY_i$
\end{itemize}
We follow remark 6.27.b in \cite{ruschendorf2013mathematical} to show that if $X\leq_{dcx}Y$ then $\sum_{i=1}^dX_i\leq_{cx}\sum_{i=1}^dY_i$.
So let $X$, $Y$ so that $X\leq_{dcx}Y$ and $f$ convex.
We want to show that  $x\mapsto f\bigl(\sum_{i=1}^dx_i\bigr)$ is directionally convex, because then $\sum_{i=1}^dX_i\leq_{cx}\sum_{i=1}^dY_i$ as wanted.
To this end let $x\in\mathbb{R}^d$, $\varepsilon,\delta>0$ and $0\leq i,j\leq d$.
Because $f$ is convex, for $a,b\in\mathbb{R}$ and $0<\theta<1$ we know that
\begin{align*}
  \theta f(a)+(1-\theta)f(b)-f(\theta a+(1-\theta)b)
  &\geq0\,,
    \intertext{and also that}
    (1-\theta) f(a)+(1-\theta)f(b)-f(\theta a+(1-\theta)b)
  &\geq0\,.
    \intertext{adding both inequalities and setting $a=\sum x_i$, $b=a+\varepsilon+\delta$ and $\theta=\frac{\varepsilon}{b-a}$ yields}
    f\Bigl(\sum x_i+\varepsilon+\delta\Bigr)-f\Bigl(\sum x_i+\varepsilon\Bigr)-f\Bigl(\sum x_i+\delta\Bigr)+f\Bigl(\sum x_i\Bigr)
  &\geq0\,.
    \intertext{By definition of $\Delta^\epsilon_j$ this means}
    \Delta^\varepsilon_j\Delta^\delta_k f\Bigl(\sum x_i\Bigr)
    &\geq0\,,
\end{align*}
so indeed $x\mapsto f(\sum x_i)$ is directionally convex.
Now since $X\leq_{dcx}Y$ this means $Ef(\sum x_i)\leq Ef(\sum Y_k)$, so that $\sum x_i\leq_{cx}\sum Y_k$, since $f\in{\cal F}_{cx}$ was chosen arbitrarily.
\pagebreak
\paragraph{Exercise 5 \textnormal{(4 Points; Bonus)}.} Show that the Markov product $A*B$ is a bivariate copula.

To this end we use proposition 1.10.
To see that $A*B$ is a bivariate copula, we need to show that it is grounded, has uniform univariate marginals and is 2-increasing.
Since $A$ and $B$ are grounded, $\int_0^1\partial_2A(0,t)\partial_1B(t,v)dt=\int_0^1\partial_2A(0,t)\partial_1B(t,v)dt=\int_0^1\partial_2A(u,t)\partial_1B(t,0)dt=0$, so $A*B$ is grounded.
To see that $A*B$ has uniform univariate marginals, we calculate $A*B(u,1)=\int_0^1\partial_2A(u,t)\partial_1B(t,1)dt=\int_0^1\partial_2A(u,t)dt=A(u,1)-A(u,0)=u$.
An analogous calculation yields ${A*B}(1,v)=v$.

To see that $A*B$ is 2-increasing, according to equation (1.14) we need to see that for all $0\leq u,v\leq 1$, $0\leq\varepsilon\leq1-u$ and $0\leq\delta\leq1-v$ we get $A*B(u+\varepsilon,v+\delta)-A*B(u,v+\delta)-A*B(u+\varepsilon,v)+A*B(u,v)\geq0$.
Employing yields
\begin{align*}
  &A*B(u+\varepsilon,v+\delta)-A*B(u,v+\delta)-A*B(u+\varepsilon,v)+A*B(u,v)\\
  =&\int_0^1\bigl(\partial_2A(u+\varepsilon,t)\partial_1B(t,v+\delta)-\partial_2A(u,t)\partial_1B(t,v+\delta)\\
  \qquad&-\partial_2A(u+\varepsilon,t)\partial_1B(t,v)+\partial_2A(u,t)\partial_1B(t,v)\bigr)dt\\
  =&\int_0^1\bigl[\partial_2\bigl(A(u+\varepsilon,t)-A(u,t)\bigr)\partial_1B(t,v+\delta)-\partial_2\bigl(A(u+\varepsilon,t)-A(u,t)\bigr)\partial_1B(t,v)\bigr]dt\\
  =&\int_0^1\bigl[\partial_2\bigl(A(u+\varepsilon,t)-A(u,t)\bigr)\partial_1\bigl(B(t,v+\delta)-B(t,v)\bigr)\bigr]dt\,.
     \intertext{
However, $\partial_2\bigl(A(u+\varepsilon,t)-A(u,t)\bigr)=\lim_{h\to0}\frac{1}{h}\bigl(A(u+\varepsilon,t+h)-A(u,t+h)-A(u+\varepsilon,t)+A(u,t)\bigr)\geq0$, because $A$ is 2-increasing.
Analogously $\partial_1\bigl(B(t,v+\delta)-B(t,v)\bigr)\geq0$.
     Thus, the integrand is positive for every $0\leq t\leq1$ so that we get}
     \geq&0\,.
\end{align*}
All together $A*B$ is a bivariate copula.

\bibliography{../../../books/wt}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
