\documentclass{article}
\usepackage[a4paper,margin=1.875in,top=1.875in,bottom=1.875in]{geometry}
\usepackage{amsmath,mathtools,bbm,amssymb,stmaryrd,wasysym}
\usepackage{mathrsfs}

\usepackage{setspace}
\doublespacing

\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{0pt} 
\pagestyle{fancy}
\lhead{Sheet 1 Evgenij}\rhead{Page \thepage}
\fancyfoot{}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,arrows}

\usepackage[numbers]{natbib}
\bibliographystyle{alphadin}
\usepackage{url}
\usepackage{hyperref}

\begin{document}

\paragraph{Exercise 1 \textnormal{(FrÃ©chet-Hoeffding bounds inequality; 4 Points)}.}
Let $C$ be a 2-dimensional copula.
Prove that for every $(u,v)$ in $[0,1]^2$,
\[
\max(u+v-1,0)\leq C(u,v)\leq\min(u,v)\,.
\]
\emph{Hint. Use the monotonicity in each component for the upper bound.
  For the lower bound use the inclusion--exclusion principle.}

We follow the proof of Theorem 2.2.3 in \cite{nelsen2006introduction}.
Let $(u,v)$ be an arbitrary point in $[0,1]^2$.
By proposition 1.10.iii $C$ is $2$-increasing, so for $\varepsilon=1-u$ we get $C\bigl((u,v)+\varepsilon(1,0)\bigr)=C(1,v)\geq C(u,v)$ and accordingly $C(u,1)\geq C(u,v)$.
By proposition 1.10.ii $C$ has uniform univariate marginals, so $C(1,v)=v$ and $C(u,1)=u$.
All together $C(u,v)\leq\min(u,v)$.
Also by proposition 1.10.i $C$ is grounded, so $0=C(0,v)\leq C\bigl((0,v)+u(1,0)\bigr)=C(u,v)$, again because $C$ is $2$-increasing.
Finally, because $C$ is 2-increasing
\begin{align*}
  0
  &\leq\Delta^{1-u}_u\Delta^{1-v}_vC(u,v)\,.
    \intertext{By definition of delta we get}
  &=\Delta^{1-u}_u\bigl(C(u,1)-C(u,v)\bigr)\\
  &=C(1,1)-C(1,v)-C(u,1)+C(u,v).
    \intertext{Because $C$ has uniform univariate marginals, we arrive at}
  &=1-v-u+C(u,v).
\end{align*}
Solving for $C(u,v)$ yields $C(u,v)\geq u+v-1$, so that all together we get $C(u,v)\geq\max(u+v-1,0)$.
\pagebreak
\paragraph{Exercise 2 \textnormal{(Survival Copula; 4 Points)}.}
For a pair $(X,Y)$ of real-valued random variables with joint distribution function $H$, the joint survival function is given by $\overline{H}(x,y)=P[X>x,Y>y]$.
The marginal distribution functions of $X$ and $Y$ are denoted by $F$ and $G$, respectively.
The corresponding survival functions are given by $\overline{F}=1-F$ and $\overline{G}=1-G$, respectively.
Let $C$ be a copula for $(X,Y)$, i.e, $C$ is a copula such that $H(x,y)=C(F(x),G(y))$.
Define the survival copula $\hat{C}(u,v)=u+v-1+C(1-u,1-v)$ and show that
\[
  \overline{H}(x,y)=\hat{C}(\overline{F}(x),\overline{G}(y))\,.
\]
Moreover, prove that $\hat{C}$ is indeed a copula.

We follow the explanation in the beginning of section 2.6 of \cite{nelsen2006introduction}.
We use the inclusion-exclusion principle, which tells us that $P(X>x,Y>y)=P(X>x)+P(Y>y)-P(\{X>x\}\cup\{Y>y\})$ so that
\begin{align*}
  \overline{H}(x,y)
  &=P(X>x)+P(Y>y)-1+P(X\leq x, Y\leq y)\,.
    \intertext{by definitions of $\overline{F}$ and $\overline{G}$ and the fact that $H(x,y)=C(F(x),G(y))$ we get}
  &=\overline{F}(x)+\overline{G}(y)-1+C(F(x),G(y))\,.
    \intertext{Finally, again by definition of $\overline{F}$ and $\overline{G}$, we get}
    &=\overline{F}(x)+\overline{G}(y)-1+C(1-\overline{F}(x),1-\overline{G}(y))\,.
\end{align*}
Now if we define the survival copula $\hat{C}$ as above, we arrive at $\overline{H}(x,y)=\hat{C}(\overline{F}(x),\overline{G}(y))$.
\pagebreak
To see that $\hat{C}$ is a copula we use proposition 1.10 and prove that it is grounded, has uniform univariate marginals and is 2-increasing.
Because $\hat{C}(u,0)=u-1+C(1-u,1)=0$ and analogously $\hat{C}(0,v)=0$, so $\hat{C}$ is grounded.
Also $\hat{C}(u,1)=u+C(1-u,0)=u$ and accordingly for $v$, so that it has uniform univariate marginals.
Finally we want to see whether $\hat{C}$ is $2$-increasing.
By definition of $\Delta^{\varepsilon_u}_u$, as already used in exercise 1, we see that
\begin{align*}
  \Delta^{\varepsilon_u}_u\Delta^{\varepsilon_v}_v\hat{C}(u,v)
  &=\hat{C}(u+\varepsilon_u,v+\varepsilon_v)-\hat{C}(u+\varepsilon_u,v)\\
  &\quad-\hat{C}(u,v+\varepsilon_v)+\hat{C}(u,v)\,.
    \intertext{By definition of the survival copula we get}
  &=C(1-u-\varepsilon_u,1-v-\varepsilon_v)-C(1-u-\varepsilon_u,1-v)\\
  &\quad-C(1-u,1-v-\varepsilon_v)+C(1-u,1-v)\,.
    \intertext{Again by definition of $\Delta^{\varepsilon}$ we arrive at}
  &=\Delta^{\varepsilon_u}_u\Delta^{\varepsilon_v}_vC(1-u-\varepsilon_u,1-v-\varepsilon_v)\geq0\,,
\end{align*}
since $C$ is a copula and thus 2-increasing.
\pagebreak
\paragraph{Exercise 3 \textnormal{(Invariance properties; 4 Points)}.}
Let $X$ and $Y$ be random variables with distribution function $F_X$ and $F_Y$, respectively.
Let $F_{XY}$ be the joint distribution function of $(X,Y)$ and $C_{XY}$ be a copula for $(X,Y)$, i.e, $F_{XY}(x,y)=C_{XY}(F_X(x),F_Y(y))$.
If $a$ and $b$ are strictly increasing and continuous on $\operatorname{Ran}(X)$ and $\operatorname{Ran}(Y)$, respectively, then $C_{XY}$ is also a copula for the random vector $(a(X),b(Y))$, i.e, the joint distribution function of $(a(X),b(Y))$ fulfills $F_{a(X)b(Y)}(x,y)=C_{XY}(F_{a(X)}(x),F_{b(Y)}(y))$.
Thus, $C_{XY}$ is invariant under strictly increasing transformations of $X$ and $Y$.

We follow the proof of theorem 2.4.3 in \cite{nelsen2006introduction}.
Because $a$ and $b$ are strictly increasing, we can apply $a^{-1}$ inside of $P$.
That is we have $F_{a(X)}(x)=P(a(X)\leq x)=P(X\leq a^{-1}(x))=F_X(a^{-1}(x))$ and accordingly $F_{b(Y)}(y)=F_Y(b^{-1}(y))$ so that for every $x,y\in\overline{\mathbb{R}}$ by Sklar's theorem we get
\begin{align*}
  C_{a(X)b(Y)}(F_{a(X)}(x),F_{b(Y)}(y))
  &=P(a(X)\leq x,b(Y)\leq y)\,,\\
  &=P(X\leq a^{-1}(x),Y\leq b^{-1}(y))\,,
    \intertext{with the same trick as above.
    Using Sklar's theorem again, we can rewrite this to}
  &=C_{XY}(F_X(a^{-1}(x)),F_Y(b^{-1}(y))\,,\\
  &=C_{XY}(F_{a(X)}(x),F_{b(Y)}(y))\,,
\end{align*}
yet again by the relation we found out in the beginning.
Now $X$ and $Y$ are continuous.
That means that $\operatorname{Ran} F_{a(X)}=F_{b(Y)}=[0,1]$ so we get the equality indeed for every $(x,y)\in[0,1]^2$ and together with $F_{a(X)b(Y)}(x,y)=C_{a(X)b(Y)}(F_{a(X)}(x),F_{b(Y)}(y))$ we get $F_{a(X)b(Y)}(x,y)=C_{XY}(F_X(x),F_Y(y))$.
\pagebreak
\paragraph{Exercise 4 \textnormal{(Distributional transform; 4 Points)}.}
Let $X$ be a real-valued random variable with distribution function $F$, and let $V\sim \operatorname{U}(0,1)$ be an independent random variable uniformly distributed on $(0,1)$.
The \emph{generalized distribution function} of $X$ is defined by
\begin{align*}
  F(x,\lambda)
  &=P(X<x)+\lambda P(X=x)\,,
  \intertext{or equivalently}
  F(x,\lambda)
  &=F(x{-})+\lambda\bigl(F(x)-F(x{-})\bigr)\,,
\end{align*}
and the \emph{generalized distribution transform} $U$ of $X$ is given by $U=F(X,V)$.

\noindent Show that $U\sim\operatorname{U}(0,1)$.

\noindent\emph{Hint: Define $p=P\bigl(X<F^{-1}(\alpha)\bigr)$ and $q=P\bigl(X=F^{-1}(\alpha)\bigr)$ and show the equality
\[
  \{U\leq \alpha\}=\{X<F^{-1}(\alpha)\}\cup\{X=F^{-1}(\alpha),p+Vq\leq \alpha\}\,.
\]
\noindent You can use without proof that $F(F^{-1}(\alpha))\leq\alpha$ and if $F$ is continuous at $F^{-1}(\alpha)$ that it holds that $F(F^{-1}(\alpha))=\alpha$.}

We follow the proof of proposition 2.1 in \cite{RUSCHENDORF20093921}.
By definition of $F$ we have $U=F(X,V)\leq\alpha$ iff $(X,V)\in\{(x,\lambda)\colon P(X<x)+\lambda P(X=x)\leq\alpha\}$.
Now consider the case that $0=q=P(X=F^{-1}(\alpha))$.
Then
\begin{align*}
  P(\{U\leq \alpha\})
  &=P(\{P(X<x)\leq\alpha\})=P(\{X<F^{-1}(\alpha)\})\\
  &=P(X\leq F^{-1}(\alpha))=F(F^{-1}(\alpha))=\alpha\,,
\end{align*}
since $F$ is continuous at $\alpha$.
Now if $q>0$, then
\begin{align*}
(X,V)&\in\{(x,\lambda)\colon P(X<x)+\lambda P(X=x)\leq\alpha\}
\intertext{is equivalent to}
(X,V)&\in\{X<F^{-1}(\alpha)\}\cup\{X=F^{-1}(\alpha),p+Vq\leq\alpha\},
\end{align*}
\emph{however also here I don't really see how this works.}
Employing we again get
\begin{align*}
  P(U\leq\alpha)
  &=P(F(X,V)\leq\alpha)\,.
    \intertext{Solving $q+V\beta\leq\alpha$ for $V$ we get}
  &=q+\beta P(V\leq(\alpha-q)/\beta)\,.
    \intertext{\emph{However I don't quite get how this works}.
    Since $V\sim\operatorname{U}(0,1)$ we get}
  &=q+\beta\frac{\alpha-q}{\beta}=\alpha\,.
\end{align*}
So that in both cases $P(U\leq\alpha)=\alpha$ and thus $U\sim\operatorname{U}(0,1)$.
\bibliography{../../../books/wt}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
